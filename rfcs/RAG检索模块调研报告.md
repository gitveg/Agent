RAG检索模块调研报告

当前，在信息检索、推荐系统和自然语言处理等多个前沿领域，将文本、图像等多模态数据高效转化为向量表示已成为核心技术。这些在高维空间中捕捉数据语义信息的向量，使得通过计算它们之间的距离来衡量相似性成为可能，从而极大地推动了相关应用的发展。随着深度学习技术的日臻成熟，向量模型在生成高质量嵌入方面取得了显著进展，与此同时，如何在海量数据集中进行高效、准确的向量检索，也已然成为业界和学界共同关注的研究焦点。

以ModelScope平台上的Qwen/Qwen3-Embedding-0.6B模型为例，我得以深入了解通义千问家族在向量模型领域的最新探索。Qwen3 Embedding系列作为专为文本嵌入和排名任务设计的最新力作，基于Qwen3系列的基础密集模型构建。该系列提供了0.6B、4B和8B等多种参数规模的文本嵌入与重排序模型。这些模型在广泛的下游应用评估中展现出卓越性能，其中8B尺寸的嵌入模型在MTEB多语言排行榜上以70.58的得分位居榜首，并在文本检索、代码检索、文本分类、文本聚类和双语文本挖掘等多个任务中均取得显著进步。Qwen3 Embedding系列的显著优势在于其全面的灵活性，从0.6B到8B的多种尺寸模型能够满足不同效率和效果的用例需求，且开发人员可以无缝结合其提供的嵌入和重排序模块。此外，嵌入模型支持灵活的向量维度定义，其中0.6B模型支持32到1024维，这通常通过在最终输出层后添加一个可配置的线性投影层或池化策略来实现，允许用户根据存储和计算需求调整向量长度。同时，嵌入和重排序模型均支持用户自定义指令，这类似于大型语言模型中的提示工程，通过在输入前添加特定指令引导模型生成更符合特定任务、语言或场景需求的高质量嵌入。

Qwen3 Embedding模型在MTEB等基准测试中与众多顶尖模型进行比较，这充分反映了当前向量模型领域激烈的竞争态势。除了Qwen3 Embedding，当前市场上还有诸多其他知名的向量模型，如NV-Embed-v2、GritLM-7B、BAAI通用嵌入、multilingual-e5-large-instruct、gte-Qwen2-1.5B/7b-Instruct、text-embedding-3-large、Cohere-embed-multilingual-v3.0、Gemini Embedding、stella\_en\_1.5B\_v5、bge-multilingual-gemma2以及ritrieve\_zh\_v1等。这些模型大多基于Transformer架构，通过在大规模文本语料上进行预训练，学习语言的深层语义表示。其中“multilingual”版本通常通过在多语言数据集上训练或对齐不同语言的表示来获得多语言能力；而“instruct”版本则通常通过指令微调来提升其在特定任务上的表现。这些模型各具特色，在不同规模、语言支持和特定任务上展现出各自的优势，共同推动了向量嵌入技术的蓬勃发展。如表1是整理出的MTEB多语言排行榜主要模型性能对比表格。

表1 MTEB多语言排行榜主要模型性能对比表格

|模型|参数量|Mean (Task)|Mean (Type)|Bitxt Mining|Class.|Clust.|Inst.|Retri.|Multi. Class.|Pair. Class.|Rerank|Retri.|STS|
| - | - | - | - | - | - | :- | :- | :- | :- | :- | :- | :- | :- |
|NV-Embed-v2|7B|56.29|49.58|57.84|57.29|40.80|1.04|18.63|78.94|63.82|56.72|71.10||
|GritLM-7B|7B|60.92|53.74|70.53|61.83|49.75|3.45|22.77|79.94|63.78|58.31|73.33||
|BGE-M3|0.6B|59.56|52.18|79.11|60.35|40.88|-3.11|20.1|80.76|62.79|54.60|74.12||
|multilingual-e5-large-instruct|0.6B|63.22|55.08|80.13|64.94|50.75|-0.40|22.91|80.86|62.61|57.12|76.81||
|gte-Qwen2-1.5B-instruct|1.5B|59.45|52.69|62.51|58.32|52.05|0.74|24.02|81.58|62.58|60.78|71.61||
|gte-Qwen2-7B-Instruct|7B|62.51|55.93|73.92|61.55|52.77|4.94|25.48|85.13|65.55|60.08|73.98||
|text-embedding-3-large|-|58.93|51.41|62.17|60.27|46.89|-2.68|22.03|79.17|63.89|59.27|71.68||
|Cohere-embed-multilingual-v3.0|-|61.12|53.23|70.50|62.95|46.89|-1.89|22.74|79.88|64.07|59.16|74.80||
|Gemini Embedding|-|68.37|59.59|79.28|71.82|54.59|5.18|29.16|83.63|65.58|67.71|79.40||
|Qwen3-Embedding-0.6B|0.6B|64.33|56.00|72.22|66.83|52.33|5.09|24.59|80.83|61.41|64.64|76.17||
|Qwen3-Embedding-4B|4B|69.45|60.86|79.36|72.33|57.15|11.56|26.77|85.05|65.08|69.60|80.86||
|Qwen3-Embedding-8B|8B|70.58|61.69|80.89|74.00|57.65|10.06|28.66|86.40|65.63|70.88|81.08||

随着向量模型的普及以及数据规模的爆炸式增长，如何高效、准确地进行向量检索显得尤为关键。在这一背景下，近期涌现出诸多先进的向量检索技术。传统的精确最近邻搜索在大规模数据集上的计算成本极高，因此近似最近邻（ANN）搜索算法已成为主流。当前，ANN算法的持续优化是研究热点之一，其中Hierarchical Navigable Small Worlds (HNSW)算法因其在速度和召回率之间的良好平衡，已成为最受欢迎和性能最优的ANN算法之一。HNSW通过构建一个多层的邻居图来实现快速搜索：顶层图的度较小，用于快速定位目标区域；底层图的度较大，用于精确搜索。其搜索过程通常采用贪婪搜索策略，从图的入口点开始，迭代地向距离查询向量更近的邻居移动，直到无法找到更近的邻居为止。其平均搜索时间复杂度接近对数级别。

除此以外，量化技术也取得了显著进展，其目的是大幅压缩索引大小并加速距离计算，主要的量化技术有乘积量化、二值量化和残差量化等。

乘积量化（PQ）将高维向量划分为若干个子向量，并为每个子向量学习一个独立的码本。原始向量通过查找码本中最近的质心来近似表示。在检索时，通过计算查询向量的子向量与码本质心之间的距离，然后组合这些距离来估计原始向量之间的距离，并且通常采用不对称距离计算，即查询向量保持浮点精度，数据库向量使用量化表示。

二值量化将浮点向量转换为二值向量，通过学习哈希函数将数据点映射到汉明空间。存储空间极小，且汉明距离计算速度极快，但通常会牺牲较多精度，如何学习更优的哈希函数以减少信息损失也是当前研究的重点。

残差量化（RQ）通过迭代地量化残差向量来逐步细化向量表示。首先对原始向量进行粗粒度量化，然后计算原始向量与量化向量之间的残差，再对残差进行量化，如此重复，直到残差足够小。这在压缩率和精度之间取得了更好的平衡。

谈到索引技术，倒排文件索引（IVF）常与PQ结合使用，其核心思想是首先通过聚类将数据集向量分配到若干个“单元”，每个单元维护一个倒排列表。在检索时，只需对与查询向量最接近的少量单元进行遍历和搜索，从而有效减少了搜索空间。

而混合检索是另一重要趋势，它基于向量相似性结合了纯粹的语义检索和BM25、SPLADE等传统的关键词检索的优势。SPLADE是一种基于掩码语言模型的稀疏检索方法，它能为文档和查询生成稀疏但可解释的权重，与传统的BM25相比在某些情况下表现更优。混合检索通过加权融合或多阶段筛选策略，实现了更全面的检索效果。在大型语言模型（LLM）的应用中，检索增强生成（RAG）范式通常就利用混合检索从知识库中获取相关文档片段，以增强LLM的生成能力。

因此可以看出向量模型与向量检索技术是当前人工智能领域的核心驱动力。Qwen3 Embedding系列模型的推出展现了大型预训练模型在生成高质量、多功能嵌入方面的强大潜力，特别是在多语言和指令感知方面取得了显著进展。同时，为了有效利用这些高质量嵌入，向量检索技术也在不断演进，通过优化ANN算法、发展混合检索、引入高效重排序以及利用专门的向量数据库等多种手段，共同推动了信息检索系统向更高效率和更高精度发展。展望未来，研究将继续聚焦于如何在保持检索性能的同时，进一步提升检索效率、降低计算资源消耗，并更好地应对复杂多样的实际应用场景。

参考文献：[https://modelscope.cn/models/Qwen/Qwen3-Embedding-0.6B](https://www.google.com/url?sa=E&q=https://modelscope.cn/models/Qwen/Qwen3-Embedding-0.6B)

